---
title: "R Statistics Practical"
author: "Christoforos Nikolaou"
date: "`r Sys.Date()`"

output:
  html_document:
    code_folding: show
    fig_caption: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
  word_document: null
  pdf_document:
    latex_engine: xelatex
link-citations: yes
bibliography: null
---

```{r}
setwd("/home/christoforos/Dropbox/github/MolBioMedClass/RMarkdown")
```

## R Practicals

In this practical class we will try to put into practice a number of statistical and bioinformatics analyses we have already discussed and presented.

### Part 1. Statistics

#### 1.1 Comparing means of distributions with t-test
We will compare the GC% of the yeast genes vs yeast non-coding sequences. We will first load them in R like this:

```{r}
gcGenes<-read.delim("../Datasets/SCgenes_gcContent.tsv", header=T, sep="\t")
gcRegSeq<-read.delim("../Datasets/SCRegSeq_gcContent.tsv", header=T, sep="\t")
```

and inspect the dataframe structures

```{r}
str(gcGenes)
head(gcRegSeq)
```

Next, we may plot the distributions of GC% values in the form of boxplots.

```{R}
boxplot(gcGenes$GC, gcRegSeq$GC, lwd=2, col="dark orange", names=c("Genes", "Reg Seqs"), main="Yeast GC%", las=1, xlab="Genome Partition", cex.main=1.8, cex.axis=1.3, cex.lab=1.4)
```

And, as a final step, we will assess the differences in the distributions' means by applying a t-test.

```{r}
t.test(gcGenes$GC, gcRegSeq$GC)
```

##### Additional Exercise

Do the same analysis (boxplot, t-test) for random samples of 100 genes vs 100 non-coding sequences from the original datasets, using R's sample function. Do the observations quantitatively and/or qualitatively change? 

#### 1.2 Comparing means of more than two distributions with ANOVA

For this (and other) exercises we will use a more complex dataset of gene expression. We will draw the normalized gene expression data from the work of [Karagianni et al., PLoS Computational Biology, 2018](https://journals.plos.org/ploscompbiol/article/authors?id=10.1371/journal.pcbi.1006933).

This includes gene expression counts for >18000 genes referring to 6 different conditions. One normal (WT), a disease transgenic model (TG) and 4 different treatment conditions for this TG model (designated as TherA-D). There are 10 biological replicates for each gene expression measurement.

Loading the data:

```{r}
gene_expression_data <- read.delim("../Datasets/GeneExpressionDataset_normalized.tsv")
```

and visualizing the structure of the dataframe 

```{r}
str(gene_expression_data)
head(gene_expression_data)
```

shows us the wealth of the dataset.

The question now is to compare the gene expression values for **each gene** across **all conditions**. We know that we can (and should) do this with ANOVA, so we need to figure out how.

Each gene is contained in one row of this dataframe. We may obtain its values across conditions with the bracket subsetting structure like this:

```{r}
gene_expression_data[1,]
```

What we need to keep in mind though, is that in order 
to apply ANOVA we need to create a dataframe where a numerical vector will be paired with a categorical vector. Here the numerical values are the gene expression values, but we don't (yet) have a categorical vector. We need to create this through a vector function like so:

```{r}
group <- c(rep("WT", 10), rep("TG", 10), rep("TherA", 10), rep("TherB", 10), rep("TherC", 10), rep("TherD", 10))
group
```

This simple vector contains the information that needs to be matched with gene expression values **for each gene**.   

We now only need to create a dataframe for the gene in question and apply ANOVA. Let's consider one particular gene (say the crucial transcriptional regulator Klf4).
First we locate its position (row) in the dataframe
```{r}
i <- which(gene_expression_data$Gene == "Klf4")
i
```

We then extract the gene expression values for all conditions for this gene.

```{r}
values <- gene_expression_data[i, 2:61]
values
```

Notice how, from the dataframe we take the i-th row and columns 2 to 61, since the 1st column doesn't hold gene expression values but simply the name of the gene. Let's now join numerical values and the categorical vector we created above in one dataframe with the known function

```{r}
genedata <- data.frame("GeneExpression"=as.numeric(values), "Group"=as.factor(group))
genedata
```

Notice the as.numeric() and as.factor() functions applied on the constituent vectors to make sure these are incorporated in the correct class.

We are now good to go with the ANOVA application, with a simple call of the aov() function, on the output of which we will apply the Tukey's "honestly significant difference" (HSD) function.

```{r}
geneaov <- aov(GeneExpression ~ Group, data = genedata)
tukeyaov <- TukeyHSD(geneaov)
tukeyaov
```

we may notice that only the first 5 rows of the output are of interest as they contain the comparisons of all conditions against TG. We can obtain them with a careful use of brackets to first get the whole table (using the $Group vector name) and then select only the first 5 rows ([1:5])

```{r}
tukey <- tukeyaov$Group[1:5,]
tukey
```

Where we see that all comparisons yield significant differences.

##### Additional exercises

1. Above we saw how we can perform ANOVA on **one particular gene**. But how would we do it for more than one?
Think of what would be required to perform this analysis on 100, 1000 or all genes of the dataset.
2. Create the ANOVA WT-TG differences for entire lists of genes.

##### Solutions/Hints (skip for now)

1. You may use a for-loop structure such as:
```{r}
for(i in 1:100){
  values <- gene_expression_data[i, 2:61]
  # etc
  # ...
  # ...
}
```

2. We first need to decide on the list. One interesting case is to focus on particular pathways. Below there is an example of how one can get the genes belonging to a particular pathway from the mouse genome.

```{r}
mouseKEGG<-readRDS("~/Dropbox/Data/GO_KEGG_Lists_RDS/kegg_pathways_list_mus_musculus.rds")
mtorgenes <- as.character(mouseKEGG$`mTOR signaling pathway - Mus musculus (house mouse)`[3][,1])
```

and then use a handy function/symbol of %in% to extract the genes in question from the dataframe

```{r}
i <- which(gene_expression_data$Gene %in% mtorgenes)
```

and then use i as a vector instead of an index. 

#### 1.3 Assessing ratios and percentages

One of the most common statistical analyses has to do with the decision on whether some combinations of two (or more) categorical values lead to more interesting co-occurrences than others. Say that you have smokers and non-smokers. Do the men smoke more or less than the women? 

Or, in a more directly biological example: consider a position in the genome in which there is variation between individuals between G (the most common allele) and A (the less frequent allele). Now, also consider that we are suspecting there is association of this allele with a certain medical condition. To this end we have genotyped individuals with and without that condition at this particular position. We thus have two categorical variables: Genotype = (common, rare allele) and Condition = (healthy, diseased). 
The data we got from 147 individuals are contained in the following dataset:

```{r}
genevardata <- read.table("../Datasets/GeneticVariationData.tsv", header=T, sep="\t")
str(genevardata)
head(genevardata)
```

The question is: Does our assumption of genetic association of the locus with this disease hold for this particular dataset?

Our approach will require two steps. First we need to create a table that holds the combinations of the two categorical values. That is, how many healthy individuals have G or A and the same for the diseased ones. We can do this directly with a useful function called table():

```{r}
varTab <- table(genevardata$Genotype, genevardata$Condition)
varTab
```

Which means that out of 78 diseased individuals, 37 had the rare allele and 31 the common one, while for the healthy individuals the situation is inversed with 49 having the common allele and 30 having the rare one.

We can visualize these differences in stacked bar plot like the one below:

```{r}
library(ggplot2)
ggplot(genevardata, aes(fill=Genotype, x=Condition, y=Genotype)) + 
    geom_bar(position="stack", stat="identity") + ggtitle("Genotype Data")
```

The second (and last) step will be to assess the significance of this discrepancy. We will do this by applying a statistical test called chi-square, or Fisher's exact test (in honour of the Ronald Fisher who first devised it). This is done like so:

```{r}
fisher.test(varTab)
```

From which we see that:
a. The odds ratio statistic is 1.94. The odds ratio is, as its name implies, the ratio of the two odds calculated as the percentages of combinations of categorical data. That is, from the varTab table we see that the odds for the rare allele are 37/31=1.194 for the diseased and 30/49=0.612 for the healthy individuals. The ratio of these two fractions is: 1.194/0.6122 ~ 1.95. This means that it is almost two times (1.95) more likely that you get the rare allele if you have the disease than if you don't. 

Is this enrichment/association significant? Yes, because: 

b. The p-value reported by the test is 0.04885, which is below the generally accepted threshold of 0.05 and
c. Because the 95% confidence interval of the test is [0.958261, 3.97442] which **doesn't contain the basal value of 1**. 

#### Additional exercises
1. Why is the basal value for Fisher's test 1 and not 0?
2. Suppose we had not one but >100 loci with suspected associations with this disease. What would we need to be mindful of while performing the same analysis on all of them?


#### 1.4 Correlation and Regression

One major type of statistical analyses that we often do have to do with quantitative associations between numerical data. We analyze them in the form of correlations.

We will see one example from genomic data with two different types of measurements that are suggested to be associated: gene expression and DNA methylation.

Below we load onto R a dataset containing relative DNA methylation and differential gene expression for >500 mouse genes in a genome-wide experiment that analyzed both transcriptomic and epigenetic data. These come from this work by [Ntougkos et al., 2017](https://acrjournals.onlinelibrary.wiley.com/doi/epdf/10.1002/art.40128). You can find a version of the dataset [here](https://github.com/christoforos-nikolaou/MolBioMedClass/blob/master/Datasets/Exp_Meth_Deg_pvalues.tsv).

```{r}
expmethdata<-read.delim("../Datasets/Exp_Meth_Deg_pvalues.tsv", header=T, sep="\t")
head(expmethdata)
str(expmethdata)
```

You see that the dataset contains a large number of variables related to the names of the genes, their positions in the genome, methylation and expression values for three different time points, as well as information on the groups they form in terms of their expression patterns. We will focus on one particular timepoint (the latest one) and obtain the values of methylation (for the promoter Regression) and the expression score for that one only. 
The desired variables are: GeneName, LateExp, meanLate_prom (corresponding to columns 1, 4 and 22 respectively).

```{r}
expmethdataLate <- expmethdata[,c(1,4,22)]
head(expmethdataLate)
```

We can plot the data on a 2-D scatterplot like this
```{r}
plot(expmethdataLate[,2:3], pch=19, col="dark grey", type="p", xlab="Gene Expression Change", ylab="Methylation Change")
```



```{r}
cor(expmethdataLate[,2:3])
```

#### 1.5 When we don't know which test to use
In one of our very first classes we talked about our lack of ability to qualitatively assess trends of continuity such as successful runs or consecutive outcomes. We had used the example of a basketball player's "hot hand" to show that runs of success (or failure) are -most of the times- due to chance if one takes into account the overall probability of an event.

We often have to deal with such problems in time series data or basically all types of data where a quantitative (or binary) outcome is a function of a linear variable (which can be space or time). In biology, and in genomics in particular, we often find ourselves dealing with data that are spatially-dependent along linear chromosomes.

In the dataset shown below we have measured DNA methylation along a 10Mb stretch of human chromosome 10. You can find the file in [this link](https://github.com/christoforos-nikolaou/MolBioMedClass/blob/master/Datasets/GenomeSeriesData.tsv)

```{r}
chrommethdata <- read.delim("../Datasets/GenomeSeriesData.tsv", header=T, sep="\t")
head(chrommethdata)
str(chrommethdata)
```

The dataset contains information on chromosome, start and end position of each segment and the Methylation score in the 4th column. This is a typical case of genomic coordinate format, which we call [bedgraph](https://genome.ucsc.edu/goldenPath/help/bedgraph.html).
In these cases it is very handy to directly plot the data in order to visualize it.

```{r}
plot(chrommethdata$start/100000000, chrommethdata$MethScore, type="l", col="olivedrab", lwd=5, xlab="Genomic Coordinates (x100Mb)", ylab="Methylation Score", main="chrom10", las=1, cex.lab=1.5, cex.main=2, cex.axis=1.3)
```

Looking at the data leads us to the suggest that something happens around one third of the interval and the methylation values drop (on average). By eye inspection alone we hypothesize that there is a boundary somewhere around 1.24 before which the values are considerably higher than after. 

We can try and visualize this boundary with a solid vertical line, like this:

```{r}
plot(chrommethdata$start/100000000, chrommethdata$MethScore, type="l", col="olivedrab", lwd=5, xlab="Genomic Coordinates (x100Mb)", ylab="Methylation Score", main="chrom10", las=1, cex.lab=1.5, cex.main=2, cex.axis=1.3)
abline(v=1.24, col="firebrick4", lty=3, lwd=3)
```

We can test this hypothesis in various ways. A simple one would be to just take **all** the values before the boundary against **all** values after it and perform a t-test.

```{r}
before<-which(chrommethdata$start<=124000000)
after<-which(chrommethdata$start>124000000)
boxplot(chrommethdata$MethScore[before], chrommethdata$MethScore[after], col=c("steelblue2", "steelblue3"), lwd=3, las=1, ylab="Methylation Score", names=c("Before", "After"))
t.test(chrommethdata$MethScore[before], chrommethdata$MethScore[after])
```

There is a clear indication of significantly higher values before the boundary compared to after. 

There is however an additional question having to do with the fluctuation of the values, as singular methylation scores on either side of the boundary can be either high or low.

A different (but related) question is:
Do stretches of high methylation scores occur more frequently before the boundary than after it?

In order to answer this question conclusively, we need to carry out a number of additional tasks that will allow us to perform a new type of statistical inference. 

The steps we will follow are:
1. We will set a baseline value according to which we will binarize the data into two categories (high/low) in the form of (success/failure). This will then allow us to check for runs of consecutive high or low values.
2. We will count the occurrences of such runs and record the number of cases we have X consecutive high (or low values) in each area of the genome.
3. We will assess these numbers statistically to see if these runs do indeed occur with higher frequencies than expected.

Below we show these steps one by one.

1. Baseline value and binarization of data.
We will use the overall mean of the values, as a simple threshold for the binarization.
```{r}
mMS<-mean(chrommethdata$MethScore)
high<-which(chrommethdata$MethScore>=mMS)
low<-which(chrommethdata$MethScore<mMS)
```

and we will translate high/low values to 1/0 for purposes of simpler computation.

```{r}
mscore<-vector(mode="numeric", length=length(chrommethdata$MethScore))
mscore[high]<-1
mscore
```

2. Calculating run lengths
Next, we want to be able to count the lengths of consecutive runs of the same value. We will use an R function for this which is called rle() (run-length encoding)

```{r}
rmscore <- rle(mscore)
rmscore
```

The output of rle is a paired vector that holds the sizes (lengths) of consecutive runs, matched with the value of the run's elements. This means that our series starts with 6 high values (1's), followed by 1 low, then 10 high values, followed by 1 low etc. We can isolate the runs of low values (0's) and check their distribution.

```{r}
lowpos <- which(unlist(rmscore[2])==0)
library(beanplot)
beanplot(unlist(rmscore[1])[lowpos], col="grey", ylab="Consecutive Low MethScores", names="Low Score")
```

(Notice that, we use the unlist() function to convert elements of a list object into the more easily handled vector format.)

3. Assessing the statistical significance of runs

Looking at runs of low values (regardless of their position) we can see that their mean value is 3.8 and their maximum is 11.

```{r}
mean(unlist(rmscore[1])[lowpos])
max(unlist(rmscore[1])[lowpos])
```

The question is: are these values high or low? Would we expect them to be higher or not. Could we get a value as high as 11 simply out of chance?

All of these questions cannot be tested against a known distribution. So we need a different approach.

### Part 2. Bioinformatics

#### 2.1 Sequence Similarity and Clustering

Our goal here will be to combine sequence similarity analysis with the clustering approaches we have discussed in class.

Histone H4, one of the core histone proteins of nucleosomal particles, is one of the most well-conserved proteins among eukaryotic organisms. In this assignment, you will have to assess **the degree of conservation** through **Needleman-Wunsch pair-wise alignment** in specific protein sequences derived from a broad group of eukaryotes.

The file you will find [here](https://github.com/christoforos-nikolaou/MolBioMedClass/blob/master/Datasets/histone4.fa) contains the H4 protein sequences from the following organisms: (Homo sapiens: HUMAN, Mus musculus: MOUSE, Drosophila melanogaster: DROME, Arabidopsis thaliana: ARATH, Bos taurus: BOVIN, Caenorhabditis elegans: CAEEL : CHICK, Rattus norvegicus: RAT, Saccharomyces cerevisiae: YEAST, Xenopus laevis: XENLA)

In the following we will:

1. Align all sequences with each other in pairs using the BLOSUM50 replacement table.
2. Indicate the alignment scores on a square table of 10X10 dimensions.

It is recommended that you use the the R::seqinr and BioConductor::Biostrings libraries.
The installation of the former is done directly by R 
```{r}
library("seqinr")
```
while for Biostrings you have to go through Bioconductor: 
```{r}
library("Biostrings")
```

The reading of the sequences can be done with the read.fasta() function of seqinr().

We will first load the sequences and check out the way they are imported in R.
```{r}
sequences<-read.fasta("../Datasets/histone4.fa")
sequences
str(sequences)
```

Looking at the sequences we see that the data are imported in character vectors. We actually need every sequence to be a continuous chunk of residues into **one single string**. This can be achieved with a function called paste(). Below we apply the function to one sequence of the object called "sequences". As **this is a list**, we use **double brackets** to assess its elements.

```{r}
paste(sequences[[1]], collapse ="")
```

the collapse="" parameter, tells paste() to *not use* any character between the joining elements. 
Below we see how we can also convert the entire string to upper case characters.

```{r}
toupper(paste(sequences[[1]], collapse =""))
```

In the following code lines we create a new list called myseqs, in which we store the joined, upper case transformed sequences of the entire dataset. This is done in **a for-loop**, a repetitive structure that is introduced with a for command and uses curly brackets to enclose loop-commands.

```{r}
myseqs<-list()
for(i in 1:length(sequences)){
  myseqs[i]<-toupper(paste(sequences[[i]], collapse=""))
}
# assigning names to myseqs, the same as in sequences
names(myseqs)<-names(sequences)
```

Our dataset now looks like this
```{r}
myseqs
```

We can now proceed with the alignment step. This will be performed pairwise, that is using two sequences every time, with the Biostrings pairwiseAlignment() function and using the substitutionMatrix = "BLOSUM50" parameter. 

See how this works with the first two sequences
```{r}
pairwiseAlignment(myseqs[[1]], myseqs[[2]], substitutionMatrix="BLOSUM50")
```

and how we can do this by only keeping the score of the alignment

```{r}
pairwiseAlignment(myseqs[[1]], myseqs[[2]], substitutionMatrix="BLOSUM50", scoreOnly=T)
```

Below we do this in one go using a **nested loop structure** that loops over all 2-sequence combinations in the dataset.

We are adding a printing command in the middle of the loop so that we keep track of what we do.

```{r}
alignMat<-matrix(0, nrow=length(myseqs), ncol=length(myseqs))
for(i in 1:length(myseqs)){
  for(j in 1:length(myseqs)){
  print(paste("Now comparing", names(myseqs)[i], " with ", names(myseqs)[j]))
    alignMat[i,j]<-pairwiseAlignment(myseqs[[i]], myseqs[[j]], substitutionMatrix="BLOSUM50", scoreOnly=T)    
  }
}
```

The final alignMat matrix contains only the scores of the sequence alignments.

```{r}
alignMat
```

The values this matrix holds are **similarity scores**. But in order to perform hierarchical clustering we require some sort of **distance score**. 

There is no rule for how to do this transformation. So we can think of or improvise various alternatives. Below I am showing one such. 

We will start by obtaining the maximum and the minimum score in the whole matrix. We will then calculate the difference of all scores against the minimum and scale this against the maximum difference of (max-min).

```{r}
maxS<-max(alignMat)
minS<-min(alignMat)
newMat<-(alignMat-minS)/(maxS-minS)
newMat
```

The newMat matrix is now _almost_ ready to be passed onto the hierarchical clustering function. As a last step we need to change its format to that of a distance with R's function as.dist().

```{r}
distMat<-as.dist(newMat)
```

As a final step we use hierarchical clustering with hclust() and then pass the result onto plot() for visualization.

```{r}
hclSeq<-hclust(distMat, method = "complete")
plot(hclSeq, labels=names(myseqs))
```

Go over the tree of the dendrogram and check if the clustering of the sequences makes sense from an evolutionary perspective.

#### 2.2 Clustering of Gene Expression Data

Having analyzed sequences we can now move on to more complex numerical datasets. 

We will see how we can use dimensionality reduction techniques on a the gene expression dataset we analyzed before with ANOVA. We will load a transposed version of this dataset in which columns have been converted to rows and vice versa, which is required for the application of PCA later on. 

```{r, Reading of Data, Selection of genes}
tqnormdata<-read.delim("../Datasets/Exp1234_raw_common18704genes_transposed_antiTNF.tsv", header=T, sep="\t")
```

Some structural subsetting is also required in order to make sure that it matches the samples we described above (as some extra samples were included in the original paper). Below we only keep the ones corresponding to the groups we saw above.

```{r}
tqnormdata<-tqnormdata[c(1:20,27:66),]
dim(tqnormdata)
```

Our goal here will be to see if our data behave in a consistent way and if our biological replicates are representative of each condition. We will do this with PCA and similar visualization approaches. 

Since the dataset contains more than 18000 genes we will restrict our analysis to the most informative genes, corresponding to the top 5% of the dataset with the highest variability. We will first calculate the variance for every gene, that is for every row of the data frame. 

See how this is easily done with an apply() function. Apply functions are **helper/wrapper functions** whose job is to _apply_ other functions in clever ways. Below we apply the var() function on the data dataframe on each column.

```{r}
colVars <- apply(tqnormdata[,2:18704], 2, FUN=var)
```

Keep in mind we have to select columns from 2 to 18704 because the first column contains gene names (character values).
Let's now find the value of colVars that corresponds to the top 5%. We can use quantiles() to do that. quantiles() splits a numerical vector in quantiles according to a vector of probabilities that we provide. (Otherwise it uses 25% percentiles by default)

```{r}
varQuants<-quantile(colVars, probs = seq(0, 1, 0.05), na.rm = FALSE)
varQuants
```

We see that the top 5% of variance values lies above 1.2564 (corresponding to the 20th element of varQuants). 

We will create a subset of the dataframe only containing the genes whose variances lie above this threshold.

```{r}
i<-which(colVars>=varQuants[20])
tdata <- tqnormdata[,i]
tdata$State <-  group
tdata$State <- factor(tdata$State) 
```

We can now apply PCA on this subset and check the outcome

```{r, PCA}
subdatapca<-prcomp(tdata[,2:936], scale=T)
summary(subdatapca)
```


```{r}
library(ggfortify)
pcols<-c("firebrick4","dark orange", "gold", "dark green", "blue", "darkorchid4")
#
p <- autoplot(subdatapca, data=tdata, colour="State", frame=T)
p+scale_fill_manual(values = pcols) + scale_color_manual(values = pcols)
```

```{r, running kmeans}
subdatakm6<-kmeans(tdata[,2:936], centers=6, iter.max = 100, nstart = 1)
conditions<-c(rep("WT",10), rep("TG",10), rep("TherA", 10), rep("TherB", 10), rep("TherC",10), rep("TherD",10))
subdatakm6contingency<-table(Cluster=as.numeric(subdatakm6$cluster),Condition=conditions)
row.names(subdatakm6contingency)<-c(paste("Cluster", seq(1:6), sep=""))
subdatakm6contingency
```


#### 2.3 Sequence motifs


For this week's assignment you are asked to perform a motif building and search analysis using a set of defined motif instances and custom-made R functions.

Your goal is to create a PWM with a set of instances of the GATA transcription factor, transform it into PSSM and then search it against a whole genome sequence.

1. You may start by downloading a set of GATA binding sequences that you will find [here](https://www.dropbox.com/s/pr51bnmp39b996a/gata.fa).

2. Once this is done, you can download the target sequence against which you will search the PWM [here](https://www.dropbox.com/s/ni6nrp0niv47kd2/test1.fa).

3. In order to build the PWM you need to read the GATA sequences using [this custom function](https://www.dropbox.com/s/2hmtvg6wn9xgjst/readfastafile.R) and then create the matrix with the [PWM function](https://www.dropbox.com/s/67815vjkrvxa72i/seqMotif.R). 
In order to use these functions you have to upload them to R using:
```{r, eval=F, echo=T}
source("nameoffunction.R")
```

This means, that you may use $readfastafile()$ as:
```{r, eval=F, echo=T}
source("readfastafile.R")
gataseqs<-readfastafile("gata.fa")
```

and then obtain the PWM with:
```{r, eval=F, echo=T}
source("seqMotif.R")
gataPWM<-seqMotif(gataseqs, drawmotif=F)
```


4. The transformation of the PWM to PSSM requires an additional random sequence collection, which you can find [here](https://www.dropbox.com/s/102g7y0ortf43dq/random.fa). You can then create the PSSM by taking the log2(ratio) of the two PWMs (one from the GATA sequences over the one from the random sequence).

5. In the next step you will use a [PSSMSearch Function](https://www.dropbox.com/s/4ssz9kg3dj9bhbt/pssmSearch.R) to search for the motif in the longer target sequence. Assuming you have created the PSSM into the table called $pssm$, the list of commands below should work:
```{r, eval=F, echo=T}
source("pssmSearch.R")
targetset<-readfastafile("test1.fa")
pssmSearch(pssm, targetseq, threshold=X)
```

The parameter X in the threshold tells the search function to return a different number of successful hits for the search. Threshold can take values from 0 to 1, with 1 returning the instances of the binding sites that have score equal to the maximum (100%) of the PSSM. A value of 0.8 will return values with scores 80% or more of the maximum and thus a greater number of hits.

6. As a last step you are asked to do the following. Perform three different searches with threshold=0.5, 0.8 and 1.0 and obtain the results in a vector. Then use this vector of positions to extract the sequences from the target sequence (test1.fa). Then store these sequences in a table (you can use "write.table") and paste them to the [WebLogo WebService](http://weblogo.berkeley.edu/logo.cgi) to obtain three different sequence logos.
Compare the three and discuss their differences. 